{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "pd.set_option('display.max_columns', 400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_prefix='done_bug_'\n",
    "file_pattern = os.path.join(\"./data/jira_trem/\", f'{file_prefix}*.csv')\n",
    "csv_files = glob.glob(file_pattern)\n",
    "\n",
    "dataframes = []\n",
    "for file in csv_files:\n",
    "    dataframes.append(pd.read_csv(file))\n",
    "df = pd.concat(dataframes, ignore_index=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 合并以 'Labels' 开头的字段为数组形式\n",
    "label_columns = [col for col in df.columns if col.startswith('Labels')]\n",
    "df['Label_List'] = df[label_columns].apply(lambda row: [x for x in row if pd.notna(x) and x != ''], axis=1)\n",
    "\n",
    "comment_fields = [col for col in df.columns if col.startswith('Comment')]\n",
    "df['Comments'] = df[comment_fields].apply(\n",
    "    lambda row: '\\n'.join(row.dropna().astype(str)) if row.notna().any() else '',\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "watcher_id_fields = [col for col in df.columns if col.startswith('Watchers Id')]\n",
    "df['WatcherIds'] = df[watcher_id_fields].apply(\n",
    "    lambda row: ','.join(row[row != 'unknown'].dropna().astype(str)) if row.notna().any() and (row != 'unknown').any() else '',\n",
    "    axis=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['Summary', 'Issue id', 'Issue Type', 'Status', 'Priority', 'Resolution', 'Assignee Id', 'Reporter Id', \n",
    "            'Creator Id', 'Created', 'Resolved', 'Affects versions', 'Fix versions', 'Due date', 'Labels', \n",
    "            'Description', 'Environment', 'Original estimate', 'Time Spent', 'Security Level', \n",
    "            'Custom field (Affected services)',\n",
    "            'Custom field (Billable)', 'Custom field (Category)', 'Custom field (Issue Origin)',\n",
    "            'Custom field (Severity)', 'Sprint', 'Custom field (Start date)', 'Custom field (Test Environment)',\n",
    "            'Parent', 'Status Category', 'Status Category Changed', 'Components', 'Label_List', 'Comments', 'WatcherIds']\n",
    "df = df[cols]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 重命名一些列名\n",
    "df =df.rename(columns={\n",
    "    'Custom field (Affected services)': 'Affected Services',\n",
    "    'Custom field (Billable)': 'Billable',\n",
    "    'Custom field (Category)': 'Category',\n",
    "    'Custom field (Issue Origin)': 'Issue Origin',\n",
    "    'Custom field (Severity)': 'Severity',\n",
    "    'Custom field (Start date)': 'Start Date',\n",
    "    'Custom field (Test Environment)': 'Test Environment'\n",
    "})\n",
    "df.dropna(subset=['Assignee Id'], inplace=True)\n",
    "# 进行时间转换\n",
    "df['Created'] = pd.to_datetime(df['Created'])\n",
    "df['Resolved'] = pd.to_datetime(df['Resolved'])\n",
    "df['Start Date'] = pd.to_datetime(df['Start Date'])\n",
    "df['Status Category Changed'] = pd.to_datetime(df['Status Category Changed'])\n",
    "# 计算时间间隔，并转换为小时\n",
    "df['Resolved Time'] = (df['Resolved'] - df['Created']).dt.total_seconds() / 60 / 60\n",
    "df['Description'] = df['Description'].fillna('')\n",
    "# 计算标题和详情的长度\n",
    "df['Summary Length'] = df['Summary'].str.len()\n",
    "df['Description Length'] = df['Description'].str.len()\n",
    "# Parent 字段填空\n",
    "df['Parent'] = df['Parent'].fillna(0)\n",
    "df['Parent'] = df['Parent'].astype(int)\n",
    "# Time Spent转换为小时\n",
    "# df['Time Spent'] = df['Time Spent']/60/60\n",
    "df['Issue Origin'] = df['Issue Origin'].fillna('Unknown')\n",
    "df['Billable'] = df['Billable'].fillna('No')\n",
    "df['Week'] = df['Created'].dt.isocalendar().week\n",
    "df['Month'] = df['Created'].dt.month\n",
    "\n",
    "df['Severity'] = df['Severity'].fillna('Unknown')\n",
    "df['Labels'] = df['Labels'].fillna('Unknown')\n",
    "df['Test Environment'] = df['Test Environment'].fillna('Unknown')\n",
    "df['Affects versions'] = df['Affects versions'].fillna('Unknown')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 删除值大部分为null的列\n",
    "df.drop(columns=['Environment', 'Components', 'Category', 'Affected Services', 'Security Level', 'Due date'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 选取有用的列\n",
    "data = df[['Issue id', 'Summary', 'Description', 'Comments', 'Assignee Id', 'Creator Id', 'Reporter Id', 'WatcherIds', 'Severity', 'Priority']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.drop_duplicates(inplace = True)\n",
    "data.reset_index(drop = True, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Assignee Id'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "文本处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string, re, nltk\n",
    "import spacy\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RegexpTokenizer\n",
    "regexp = RegexpTokenizer(\"[\\w']+\")\n",
    "\n",
    "# 转换为小写\n",
    "def convert_to_lowercase(text):\n",
    "    return text.lower()\n",
    "\n",
    "# 去除文本两边空格\n",
    "def strip_text(text):\n",
    "    return text.strip()\n",
    "\n",
    "# 移除标点符号\n",
    "def remove_punctuation(text):\n",
    "    punct_str = string.punctuation\n",
    "    punct_str = punct_str.replace(\"'\", \"\")\n",
    "    return re.sub(f\"[{re.escape(punct_str)}]\", \" \", text)\n",
    "\n",
    "# 移除标题的标点符号\n",
    "def remove_summary_punctuation(text):\n",
    "    punct_str = string.punctuation\n",
    "    punct_str = punct_str.replace(\"'\", \"\")\n",
    "    punct_str = punct_str.replace(\"_\", \"\")\n",
    "    return re.sub(f\"[{re.escape(punct_str)}]\", \" \", text)\n",
    "\n",
    "# 移除数字token\n",
    "def remove_number_token(text):\n",
    "    words = text.split()\n",
    "    # 过滤掉纯数字的词\n",
    "    filtered_words = [word for word in words if not re.match(r'^\\d+$', word)]\n",
    "    # 将词按空格合并成句子\n",
    "    combined_sentence = ' '.join(filtered_words).strip()\n",
    "    return combined_sentence\n",
    "\n",
    "# 移除html标签\n",
    "def remove_html(text):\n",
    "    html = re.compile(r'<.*?>')\n",
    "    return html.sub(' ', text)\n",
    "\n",
    "# 移除表情\n",
    "def remove_emoji(text):\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                           u\"\\U00002702-\\U000027B0\"\n",
    "                           u\"\\U000024C2-\\U0001F251\"\n",
    "                           \"]+\", flags = re.UNICODE)\n",
    "    return emoji_pattern.sub(' ', text)\n",
    "\n",
    "# 移除http链接\n",
    "def remove_http(text):\n",
    "    http = \"https?://\\S+|www\\.\\S+\" # matching strings beginning with http (but not just \"http\")\n",
    "    pattern = r\"({})\".format(http) # creating pattern\n",
    "    return re.sub(pattern, \" \", text)\n",
    "\n",
    "# Dictionary of acronyms\n",
    "acronyms_url = './data/english_acronyms.json'\n",
    "acronyms_dict = pd.read_json(acronyms_url, typ = 'series')\n",
    "acronyms_list = list(acronyms_dict.keys())\n",
    "\n",
    "# remove html tags\n",
    "def remove_html(text):\n",
    "    html = re.compile(r'<.*?>')\n",
    "    return html.sub(' ', text)\n",
    "\n",
    "# 移除文本中包含的image tag\n",
    "def remove_image_tags(text):\n",
    "    # Define the regular expression pattern to match the image tags\n",
    "    pattern = re.compile(r'!.*?!')\n",
    "    cleaned_text = pattern.sub(' ', text)\n",
    "    return cleaned_text\n",
    "\n",
    "# 移除文本中{}的内容\n",
    "def remove_bracket(text):\n",
    "    pattern = re.compile(r'\\{.*?\\}')\n",
    "    return pattern.sub(' ', text)\n",
    "\n",
    "# 移除文本中||的内容\n",
    "def remove_table(text):\n",
    "    pattern = re.compile(r'\\|.*?\\|')\n",
    "    return pattern.sub(' ', text)\n",
    "\n",
    "# 移除文本中**的内容\n",
    "def remove_star(text):\n",
    "    pattern = re.compile(r'\\*.*?\\*')\n",
    "    return pattern.sub(' ', text)\n",
    "\n",
    "# convert contractions in a text\n",
    "def convert_acronyms(text):\n",
    "    words = []\n",
    "    for word in regexp.tokenize(text):\n",
    "        if word in acronyms_list:\n",
    "            words = words + acronyms_dict[word].split()\n",
    "        else:\n",
    "            words = words + word.split()\n",
    "    \n",
    "    text_converted = \" \".join(words)\n",
    "    return text_converted\n",
    "\n",
    "# Dictionary of contractions\n",
    "contractions_url = './data/english_contractions.json'\n",
    "contractions_dict = pd.read_json(contractions_url, typ = 'series')\n",
    "# List of contractions\n",
    "contractions_list = list(contractions_dict.keys())\n",
    "\n",
    "# convert contractions in a text\n",
    "def convert_contractions(text):\n",
    "    words = []\n",
    "    for word in regexp.tokenize(text):\n",
    "        if word in contractions_list:\n",
    "            words = words + contractions_dict[word].split()\n",
    "        else:\n",
    "            words = words + word.split()\n",
    "    \n",
    "    text_converted = \" \".join(words)\n",
    "    return text_converted\n",
    "\n",
    "# 移除停用词\n",
    "def remove_stopwords(text):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    \n",
    "    # 使用正则表达式 tokenizer 处理缩写和标点\n",
    "    tokenizer = RegexpTokenizer(r'\\w+\\'?\\w+|\\w+')\n",
    "    words = tokenizer.tokenize(text)\n",
    "    \n",
    "    filtered_words = [word for word in words if word.lower() not in stop_words]\n",
    "    return ' '.join(filtered_words)\n",
    "\n",
    "# Stemming 词干提取，stemming. 如 \"running\", \"runner\" 会被转换成 \"run\".\n",
    "stemmer = PorterStemmer()\n",
    "def text_stemmer(text):\n",
    "    text_stem = \" \".join([stemmer.stem(word) for word in regexp.tokenize(text)])\n",
    "    return text_stem\n",
    "\n",
    "# Lemmatization 词形还原. 如Better被还原为good\n",
    "spacy_lemmatizer = spacy.load(\"en_core_web_sm\", disable = ['parser', 'ner'])\n",
    "#lemmatizer = WordNetLemmatizer()\n",
    "def text_lemmatizer(text):\n",
    "    text_spacy = \" \".join([token.lemma_ for token in spacy_lemmatizer(text)])\n",
    "    #text_wordnet = \" \".join([lemmatizer.lemmatize(word) for word in word_tokenize(text)]) # regexp.tokenize(text)\n",
    "    return text_spacy\n",
    "    #return text_wordnet\n",
    "\n",
    "# 移除非字母的词\n",
    "def discard_non_alpha(text):\n",
    "    word_list_non_alpha = [word for word in regexp.tokenize(text) if word.isalpha()]\n",
    "    text_non_alpha = \" \".join(word_list_non_alpha)\n",
    "    return text_non_alpha\n",
    "\n",
    "# 根据词性过滤单词, 如过滤连词(conjunctions), 介词(prepositions)。保留名词(nouns)、形容词和动词\n",
    "def keep_pos(text):\n",
    "    tokens = regexp.tokenize(text)\n",
    "    tokens_tagged = nltk.pos_tag(tokens)\n",
    "    #keep_tags = ['NN', 'NNS', 'NNP', 'NNPS', 'FW']\n",
    "    keep_tags = ['JJ', 'JJR', 'JJS', 'MD', 'NN', 'NNS', 'NNP', 'NNPS', 'FW', 'RB', 'RBR', 'RBS', 'VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ']\n",
    "    keep_words = [x[0] for x in tokens_tagged if x[1] in keep_tags]\n",
    "    return \" \".join(keep_words)\n",
    "\n",
    "# Additional stopwords\n",
    "alphabets = [\"a\", \"b\", \"c\", \"d\", \"e\", \"f\", \"g\", \"h\", \"i\", \"j\", \"k\", \"l\", \"m\", \"n\", \"o\", \"p\", \"q\", \"r\", \"s\", \"t\", \"u\", \"v\", \"w\", \"x\", \"y\", \"z\"]\n",
    "prepositions = [\"about\", \"above\", \"across\", \"after\", \"against\", \"among\", \"around\", \"at\", \"before\", \"behind\", \"below\", \"beside\", \"between\", \"by\", \"down\", \"during\", \"for\", \"from\", \"in\", \"inside\", \"into\", \"near\", \"of\", \"off\", \"on\", \"out\", \"over\", \"through\", \"to\", \"toward\", \"under\", \"up\", \"with\"]\n",
    "prepositions_less_common = [\"aboard\", \"along\", \"amid\", \"as\", \"beneath\", \"beyond\", \"but\", \"concerning\", \"considering\", \"despite\", \"except\", \"following\", \"like\", \"minus\", \"onto\", \"outside\", \"per\", \"plus\", \"regarding\", \"round\", \"since\", \"than\", \"till\", \"underneath\", \"unlike\", \"until\", \"upon\", \"versus\", \"via\", \"within\", \"without\"]\n",
    "coordinating_conjunctions = [\"and\", \"but\", \"for\", \"nor\", \"or\", \"so\", \"and\", \"yet\"]\n",
    "correlative_conjunctions = [\"both\", \"and\", \"either\", \"or\", \"neither\", \"nor\", \"not\", \"only\", \"but\", \"whether\", \"or\"]\n",
    "subordinating_conjunctions = [\"after\", \"although\", \"as\", \"as if\", \"as long as\", \"as much as\", \"as soon as\", \"as though\", \"because\", \"before\", \"by the time\", \"even if\", \"even though\", \"if\", \"in order that\", \"in case\", \"in the event that\", \"lest\", \"now that\", \"once\", \"only\", \"only if\", \"provided that\", \"since\", \"so\", \"supposing\", \"that\", \"than\", \"though\", \"till\", \"unless\", \"until\", \"when\", \"whenever\", \"where\", \"whereas\", \"wherever\", \"whether or not\", \"while\"]\n",
    "others = [\"ã\", \"å\", \"ì\", \"û\", \"ûªm\", \"ûó\", \"ûò\", \"ìñ\", \"ûªre\", \"ûªve\", \"ûª\", \"ûªs\", \"ûówe\"]\n",
    "custom_words = [\"thank\", \"hi\", \"hello\", \"regard\", \"issue\", \"please\", \"cc\"]\n",
    "additional_stops = alphabets + prepositions + prepositions_less_common + coordinating_conjunctions + correlative_conjunctions + subordinating_conjunctions + others + custom_words\n",
    "\n",
    "def remove_additional_stopwords(text):\n",
    "    return \" \".join([word for word in regexp.tokenize(text) if word not in additional_stops])\n",
    "\n",
    "def clean_text(text):\n",
    "    # 按换行符分割文本\n",
    "    lines = text.split('\\n')\n",
    "    # 过滤以#开始的句子\n",
    "    filtered_lines = [line for line in lines if not line.startswith(('#', '*'))]\n",
    "    # 将句子按空格合并为一个句子\n",
    "    combined_sentence = ' '.join(filtered_lines).strip()\n",
    "    return combined_sentence\n",
    "\n",
    "def clean_log(text):\n",
    "    # 按换行符分割文本\n",
    "    lines = text.split('\\n')\n",
    "    # 正则表达式匹配时间日期格式\n",
    "    date_pattern = r'^\\d{4}-\\d{2}-\\d{2} \\d{2}:\\d{2}:\\d{2}'\n",
    "    # 过滤以时间日期开头的句子\n",
    "    filtered_lines = [line for line in lines if not re.match(date_pattern, line)]\n",
    "    # 将句子按空格合并为一个句子\n",
    "    combined_sentence = ' '.join(filtered_lines).strip()\n",
    "    return combined_sentence\n",
    "\n",
    "def remove_comment_header(text):\n",
    "    pattern = re.compile(r'(\\d{2}/[A-Za-z]{3}/\\d{2} \\d{1,2}:\\d{2} [APM]+);([0-9a-fA-F:.-]+)')\n",
    "    return pattern.sub(' ', text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 处理标题\n",
    "def summary_normalizer(text):\n",
    "    text = strip_text(text)\n",
    "    text = convert_to_lowercase(text)\n",
    "    text = remove_punctuation(text)\n",
    "    text = remove_stopwords(text)\n",
    "    text = remove_number_token(text)\n",
    "    text = discard_non_alpha(text)\n",
    "    text = keep_pos(text)\n",
    "    text = remove_additional_stopwords(text)\n",
    "    text = strip_text(text)\n",
    "    text = re.sub(' +', ' ', text)\n",
    "    return text\n",
    "\n",
    "# 处理内容\n",
    "def description_normalizer(text):\n",
    "    text = convert_to_lowercase(text)\n",
    "    text = strip_text(text)\n",
    "    text = re.sub('\\n', ' ', text) # converting text to one line\n",
    "    text = re.sub('\\[.*?\\]', ' ', text) # removing square brackets\n",
    "    text = remove_http(text)\n",
    "    text = remove_image_tags(text)\n",
    "    text = remove_bracket(text)\n",
    "    text = remove_table(text)\n",
    "#    text = remove_star(text)\n",
    "    text = remove_punctuation(text)\n",
    "    text = remove_html(text)\n",
    "    text = remove_emoji(text)\n",
    "    text = convert_acronyms(text)\n",
    "    text = convert_contractions(text)\n",
    "    text = remove_stopwords(text)\n",
    "    text = text_lemmatizer(text)\n",
    "    text = discard_non_alpha(text)\n",
    "    text = keep_pos(text)\n",
    "    text = remove_additional_stopwords(text)\n",
    "    text = re.sub(' +', ' ', text)  # replace multiple spaces with a single space\n",
    "    text = strip_text(text)\n",
    "    return text\n",
    "\n",
    "# 处理内容\n",
    "def comment_normalizer(text):\n",
    "    text = remove_comment_header(text)\n",
    "    text = convert_to_lowercase(text)\n",
    "    text = strip_text(text)\n",
    "    text = re.sub('\\n', ' ', text) # converting text to one line\n",
    "    text = re.sub('\\[.*?\\]', ' ', text) # removing square brackets\n",
    "    text = remove_http(text)\n",
    "    text = remove_image_tags(text)\n",
    "    text = remove_bracket(text)\n",
    "    text = remove_table(text)\n",
    "#    text = remove_star(text)\n",
    "    text = remove_punctuation(text)\n",
    "    text = remove_html(text)\n",
    "    text = remove_emoji(text)\n",
    "    text = convert_acronyms(text)\n",
    "    text = convert_contractions(text)\n",
    "    text = remove_stopwords(text)\n",
    "    text = text_lemmatizer(text)\n",
    "    text = discard_non_alpha(text)\n",
    "    text = keep_pos(text)\n",
    "    text = remove_additional_stopwords(text)\n",
    "    text = re.sub(' +', ' ', text)  # replace multiple spaces with a single space\n",
    "    text = strip_text(text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['normalized_summary'] = data['Summary'].apply(summary_normalizer)\n",
    "data['normalized_description'] = data['Description'].apply(description_normalizer)\n",
    "#data['normalized_comment'] = data['Comments'].apply(comment_normalizer)\n",
    "# Text列包含标题和描述以便后面进行一起处理\n",
    "data['normalized_text'] = data['normalized_summary'] + ' ' + data['normalized_description']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data[['normalized_comment','Comments']].to_csv('/home/ryan/Downloads/comments.csv', index=False, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from mlxtend.frequent_patterns import apriori, association_rules, fpgrowth\n",
    "# from scipy.sparse import csr_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "columns cannot be a set",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_597022/177537044.py\u001b[0m in \u001b[0;36m?\u001b[0;34m()\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m \u001b[0mvectorizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCountVectorizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mngram_range\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvectorizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'normalized_text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;31m# 创建词语的one-hot编码矩阵\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ds/lib/python3.8/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[1;32m    671\u001b[0m         \u001b[0;31m# GH47215\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    672\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mindex\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    673\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"index cannot be a set\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    674\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcolumns\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 675\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"columns cannot be a set\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    676\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    677\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcopy\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    678\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: columns cannot be a set"
     ]
    }
   ],
   "source": [
    "vectorizer = CountVectorizer(ngram_range=(1, 2))\n",
    "X = vectorizer.fit_transform(data['normalized_text'])\n",
    "\n",
    "# 创建词语的one-hot编码矩阵\n",
    "word_df = pd.DataFrame(X.toarray(), columns=vectorizer.get_feature_names_out())\n",
    "\n",
    "# 处理用户字段\n",
    "def process_users(row):\n",
    "    users = set()\n",
    "    users.update([row['Assignee Id'], row['Creator Id'], row['Reporter Id']])\n",
    "    # 分割WatcherIds字段\n",
    "    if pd.notna(row['WatcherIds']):\n",
    "        users.update(row['WatcherIds'].split(','))\n",
    "    return list(users)\n",
    "\n",
    "\n",
    "data['Users'] = data.apply(process_users, axis=1)\n",
    "\n",
    "# 生成用户one-hot编码\n",
    "user_set = set(data['Users'].apply(pd.Series).stack().unique())\n",
    "user_df = pd.DataFrame(0, index=data.index, columns=user_set)\n",
    "\n",
    "\n",
    "# 遍历每一行, 设定用户字段的one-hot编码\n",
    "for idx, users in data['Users'].items():\n",
    "    for user in users:\n",
    "        user_df.loc[idx, user] = 1\n",
    "\n",
    "# 只选择词汇和用户矩阵，分别计算频繁项集和关联规则\n",
    "# 避免合并词汇和用户矩阵，降低内存消耗\n",
    "word_user_df = pd.concat([word_df, user_df], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 生成频繁项集\n",
    "frequent_itemsets = fpgrowth(word_user_df, min_support=0.05, use_colnames=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 生成关联规则 (从词语到用户)\n",
    "rules = association_rules(frequent_itemsets, metric=\"lift\", min_threshold=1)\n",
    "\n",
    "# 筛选出 word -> user 关联规则\n",
    "word_to_user_rules = rules[rules['antecedents'].apply(lambda x: any(item in vectorizer.get_feature_names_out() for item in x)) &\n",
    "                           rules['consequents'].apply(lambda x: any(user in user_df.columns for user in x))]\n",
    "\n",
    "# 输出结果\n",
    "print(word_to_user_rules)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ds",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
