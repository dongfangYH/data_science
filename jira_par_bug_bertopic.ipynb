{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "pd.set_option('display.max_columns', 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_prefix='JIRA_'\n",
    "file_pattern = os.path.join(\"./data/jira_par/\", f'{file_prefix}*.csv')\n",
    "csv_files = glob.glob(file_pattern)\n",
    "\n",
    "dataframes = []\n",
    "for file in csv_files:\n",
    "    dataframes.append(pd.read_csv(file))\n",
    "df = pd.concat(dataframes, ignore_index=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 合并以 'Labels' 开头的字段为数组形式\n",
    "label_columns = [col for col in df.columns if col.startswith('Labels')]\n",
    "df['Label_List'] = df[label_columns].apply(lambda row: [x for x in row if pd.notna(x) and x != ''], axis=1)\n",
    "\n",
    "# 合并以 'Components' 开头的字段为数组形式\n",
    "component_columns = [col for col in df.columns if col.startswith('Components')]\n",
    "df['Component_List'] = df[label_columns].apply(lambda row: [x for x in row if pd.notna(x) and x != ''], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['Summary', 'Issue id', 'Issue Type', 'Status', 'Priority', 'Resolution', 'Assignee Id', 'Reporter Id', \n",
    "        'Creator Id', 'Created', 'Resolved', 'Affects versions', 'Components',\n",
    "        'Description',  'Custom field (Product Owner (PO))', 'Status Category', 'Custom field (Requested From:)', 'Label_List','Component_List']\n",
    "df = df[cols]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 重命名一些列名\n",
    "df =df.rename(columns={\n",
    "    'Custom field (Product Owner (PO))': 'Product Owner',\n",
    "    'Custom field (Requested From:)': 'Requested From'\n",
    "})\n",
    "df.dropna(subset=['Assignee Id', 'Description', 'Product Owner'], inplace=True)\n",
    "# 进行时间转换\n",
    "df['Created'] = pd.to_datetime(df['Created'])\n",
    "df['Resolved'] = pd.to_datetime(df['Resolved'])\n",
    "# 计算标题和详情的长度\n",
    "df['Summary Length'] = df['Summary'].str.len()\n",
    "df['Description Length'] = df['Description'].str.len()\n",
    "#df['Week'] = df['Created'].dt.isocalendar().week\n",
    "#df['Month'] = df['Created'].dt.month\n",
    "df['Create Date'] = df['Created'].dt.strftime('%Y%m%d').astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = df[df['Status Category'] != 'To Do']\n",
    "data = data[data['Summary Length'] >= 10]\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 选取有用的列\n",
    "data = data[['Summary', 'Description', 'Assignee Id', 'Reporter Id', 'Components', 'Priority', 'Issue Type', 'Create Date']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Assignee Id'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "文本处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string, re, nltk\n",
    "import spacy\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RegexpTokenizer\n",
    "regexp = RegexpTokenizer(\"[\\w']+\")\n",
    "\n",
    "# 转换为小写\n",
    "def convert_to_lowercase(text):\n",
    "    return text.lower()\n",
    "\n",
    "# 去除文本两边空格\n",
    "def strip_text(text):\n",
    "    return text.strip()\n",
    "\n",
    "# 移除标点符号\n",
    "def remove_punctuation(text):\n",
    "    punct_str = string.punctuation\n",
    "    punct_str = punct_str.replace(\"'\", \"\")\n",
    "    return re.sub(f\"[{re.escape(punct_str)}]\", \" \", text)\n",
    "\n",
    "# 移除标题的标点符号\n",
    "def remove_summary_punctuation(text):\n",
    "    punct_str = string.punctuation\n",
    "    punct_str = punct_str.replace(\"'\", \"\")\n",
    "    punct_str = punct_str.replace(\"_\", \"\")\n",
    "    return re.sub(f\"[{re.escape(punct_str)}]\", \" \", text)\n",
    "\n",
    "# 移除数字token\n",
    "def remove_number_token(text):\n",
    "    words = text.split()\n",
    "    # 过滤掉纯数字的词\n",
    "    filtered_words = [word for word in words if not re.match(r'^\\d+$', word)]\n",
    "    # 将词按空格合并成句子\n",
    "    combined_sentence = ' '.join(filtered_words).strip()\n",
    "    return combined_sentence\n",
    "\n",
    "# 移除html标签\n",
    "def remove_html(text):\n",
    "    html = re.compile(r'<.*?>')\n",
    "    return html.sub(' ', text)\n",
    "\n",
    "# 移除表情\n",
    "def remove_emoji(text):\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                           u\"\\U00002702-\\U000027B0\"\n",
    "                           u\"\\U000024C2-\\U0001F251\"\n",
    "                           \"]+\", flags = re.UNICODE)\n",
    "    return emoji_pattern.sub(' ', text)\n",
    "\n",
    "# 移除http链接\n",
    "def remove_http(text):\n",
    "    http = \"https?://\\S+|www\\.\\S+\" # matching strings beginning with http (but not just \"http\")\n",
    "    pattern = r\"({})\".format(http) # creating pattern\n",
    "    return re.sub(pattern, \" \", text)\n",
    "\n",
    "# Dictionary of acronyms\n",
    "acronyms_url = './data/english_acronyms.json'\n",
    "acronyms_dict = pd.read_json(acronyms_url, typ = 'series')\n",
    "acronyms_list = list(acronyms_dict.keys())\n",
    "\n",
    "# remove html tags\n",
    "def remove_html(text):\n",
    "    html = re.compile(r'<.*?>')\n",
    "    return html.sub(' ', text)\n",
    "\n",
    "# 移除文本中包含的image tag\n",
    "def remove_image_tags(text):\n",
    "    # Define the regular expression pattern to match the image tags\n",
    "    pattern = re.compile(r'!.*?!')\n",
    "    cleaned_text = pattern.sub(' ', text)\n",
    "    return cleaned_text\n",
    "\n",
    "# 移除文本中{}的内容\n",
    "def remove_bracket(text):\n",
    "    pattern = re.compile(r'\\{.*?\\}')\n",
    "    return pattern.sub(' ', text)\n",
    "\n",
    "# 移除文本中||的内容\n",
    "def remove_table(text):\n",
    "    pattern = re.compile(r'\\|.*?\\|')\n",
    "    return pattern.sub(' ', text)\n",
    "\n",
    "# 移除文本中**的内容\n",
    "def remove_star(text):\n",
    "    pattern = re.compile(r'\\*.*?\\*')\n",
    "    return pattern.sub(' ', text)\n",
    "\n",
    "# convert contractions in a text\n",
    "def convert_acronyms(text):\n",
    "    words = []\n",
    "    for word in regexp.tokenize(text):\n",
    "        if word in acronyms_list:\n",
    "            words = words + acronyms_dict[word].split()\n",
    "        else:\n",
    "            words = words + word.split()\n",
    "    \n",
    "    text_converted = \" \".join(words)\n",
    "    return text_converted\n",
    "\n",
    "# Dictionary of contractions\n",
    "contractions_url = './data/english_contractions.json'\n",
    "contractions_dict = pd.read_json(contractions_url, typ = 'series')\n",
    "# List of contractions\n",
    "contractions_list = list(contractions_dict.keys())\n",
    "\n",
    "# convert contractions in a text\n",
    "def convert_contractions(text):\n",
    "    words = []\n",
    "    for word in regexp.tokenize(text):\n",
    "        if word in contractions_list:\n",
    "            words = words + contractions_dict[word].split()\n",
    "        else:\n",
    "            words = words + word.split()\n",
    "    \n",
    "    text_converted = \" \".join(words)\n",
    "    return text_converted\n",
    "\n",
    "# 移除停用词\n",
    "def remove_stopwords(text):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    \n",
    "    # 使用正则表达式 tokenizer 处理缩写和标点\n",
    "    tokenizer = RegexpTokenizer(r'\\w+\\'?\\w+|\\w+')\n",
    "    words = tokenizer.tokenize(text)\n",
    "    \n",
    "    filtered_words = [word for word in words if word.lower() not in stop_words]\n",
    "    return ' '.join(filtered_words)\n",
    "\n",
    "# Stemming 词干提取，stemming. 如 \"running\", \"runner\" 会被转换成 \"run\".\n",
    "stemmer = PorterStemmer()\n",
    "def text_stemmer(text):\n",
    "    text_stem = \" \".join([stemmer.stem(word) for word in regexp.tokenize(text)])\n",
    "    return text_stem\n",
    "\n",
    "# Lemmatization 词形还原. 如Better被还原为good\n",
    "spacy_lemmatizer = spacy.load(\"en_core_web_sm\", disable = ['parser', 'ner'])\n",
    "#lemmatizer = WordNetLemmatizer()\n",
    "def text_lemmatizer(text):\n",
    "    text_spacy = \" \".join([token.lemma_ for token in spacy_lemmatizer(text)])\n",
    "    #text_wordnet = \" \".join([lemmatizer.lemmatize(word) for word in word_tokenize(text)]) # regexp.tokenize(text)\n",
    "    return text_spacy\n",
    "    #return text_wordnet\n",
    "\n",
    "# 移除非字母的词\n",
    "def discard_non_alpha(text):\n",
    "    word_list_non_alpha = [word for word in regexp.tokenize(text) if word.isalpha()]\n",
    "    text_non_alpha = \" \".join(word_list_non_alpha)\n",
    "    return text_non_alpha\n",
    "\n",
    "# 根据词性过滤单词, 如过滤连词(conjunctions), 介词(prepositions)。保留名词(nouns)、形容词和动词\n",
    "def keep_pos(text):\n",
    "    tokens = regexp.tokenize(text)\n",
    "    tokens_tagged = nltk.pos_tag(tokens)\n",
    "    #keep_tags = ['NN', 'NNS', 'NNP', 'NNPS', 'FW']\n",
    "    keep_tags = ['JJ', 'JJR', 'JJS', 'MD', 'NN', 'NNS', 'NNP', 'NNPS', 'FW', 'RB', 'RBR', 'RBS', 'VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ']\n",
    "    keep_words = [x[0] for x in tokens_tagged if x[1] in keep_tags]\n",
    "    return \" \".join(keep_words)\n",
    "\n",
    "# Additional stopwords\n",
    "alphabets = [\"a\", \"b\", \"c\", \"d\", \"e\", \"f\", \"g\", \"h\", \"i\", \"j\", \"k\", \"l\", \"m\", \"n\", \"o\", \"p\", \"q\", \"r\", \"s\", \"t\", \"u\", \"v\", \"w\", \"x\", \"y\", \"z\"]\n",
    "prepositions = [\"about\", \"above\", \"across\", \"after\", \"against\", \"among\", \"around\", \"at\", \"before\", \"behind\", \"below\", \"beside\", \"between\", \"by\", \"down\", \"during\", \"for\", \"from\", \"in\", \"inside\", \"into\", \"near\", \"of\", \"off\", \"on\", \"out\", \"over\", \"through\", \"to\", \"toward\", \"under\", \"up\", \"with\"]\n",
    "prepositions_less_common = [\"aboard\", \"along\", \"amid\", \"as\", \"beneath\", \"beyond\", \"but\", \"concerning\", \"considering\", \"despite\", \"except\", \"following\", \"like\", \"minus\", \"onto\", \"outside\", \"per\", \"plus\", \"regarding\", \"round\", \"since\", \"than\", \"till\", \"underneath\", \"unlike\", \"until\", \"upon\", \"versus\", \"via\", \"within\", \"without\"]\n",
    "coordinating_conjunctions = [\"and\", \"but\", \"for\", \"nor\", \"or\", \"so\", \"and\", \"yet\"]\n",
    "correlative_conjunctions = [\"both\", \"and\", \"either\", \"or\", \"neither\", \"nor\", \"not\", \"only\", \"but\", \"whether\", \"or\"]\n",
    "subordinating_conjunctions = [\"after\", \"although\", \"as\", \"as if\", \"as long as\", \"as much as\", \"as soon as\", \"as though\", \"because\", \"before\", \"by the time\", \"even if\", \"even though\", \"if\", \"in order that\", \"in case\", \"in the event that\", \"lest\", \"now that\", \"once\", \"only\", \"only if\", \"provided that\", \"since\", \"so\", \"supposing\", \"that\", \"than\", \"though\", \"till\", \"unless\", \"until\", \"when\", \"whenever\", \"where\", \"whereas\", \"wherever\", \"whether or not\", \"while\"]\n",
    "others = [\"ã\", \"å\", \"ì\", \"û\", \"ûªm\", \"ûó\", \"ûò\", \"ìñ\", \"ûªre\", \"ûªve\", \"ûª\", \"ûªs\", \"ûówe\"]\n",
    "custom_words = [\"thank\", \"hi\", \"hello\", \"regard\", \"issue\", \"please\", \"cc\"]\n",
    "additional_stops = alphabets + prepositions + prepositions_less_common + coordinating_conjunctions + correlative_conjunctions + subordinating_conjunctions + others + custom_words\n",
    "\n",
    "def remove_additional_stopwords(text):\n",
    "    return \" \".join([word for word in regexp.tokenize(text) if word not in additional_stops])\n",
    "\n",
    "def clean_text(text):\n",
    "    # 按换行符分割文本\n",
    "    lines = text.split('\\n')\n",
    "    # 过滤以#开始的句子\n",
    "    filtered_lines = [line for line in lines if not line.startswith(('#', '*'))]\n",
    "    # 将句子按空格合并为一个句子\n",
    "    combined_sentence = ' '.join(filtered_lines).strip()\n",
    "    return combined_sentence\n",
    "\n",
    "def clean_log(text):\n",
    "    # 按换行符分割文本\n",
    "    lines = text.split('\\n')\n",
    "    # 正则表达式匹配时间日期格式\n",
    "    date_pattern = r'^\\d{4}-\\d{2}-\\d{2} \\d{2}:\\d{2}:\\d{2}'\n",
    "    # 过滤以时间日期开头的句子\n",
    "    filtered_lines = [line for line in lines if not re.match(date_pattern, line)]\n",
    "    # 将句子按空格合并为一个句子\n",
    "    combined_sentence = ' '.join(filtered_lines).strip()\n",
    "    return combined_sentence\n",
    "\n",
    "def remove_comment_header(text):\n",
    "    pattern = re.compile(r'(\\d{2}/[A-Za-z]{3}/\\d{2} \\d{1,2}:\\d{2} [APM]+);([0-9a-fA-F:.-]+)')\n",
    "    return pattern.sub(' ', text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 处理标题\n",
    "def summary_normalizer(text):\n",
    "    text = strip_text(text)\n",
    "    #text = convert_to_lowercase(text)\n",
    "#    text = remove_punctuation(text)\n",
    "    text = remove_stopwords(text)\n",
    "    text = remove_number_token(text)\n",
    "#    text = discard_non_alpha(text)\n",
    "#    text = keep_pos(text)\n",
    "#    text = remove_additional_stopwords(text)\n",
    "    text = strip_text(text)\n",
    "    text = re.sub(' +', ' ', text)\n",
    "    return text\n",
    "\n",
    "# 处理内容\n",
    "def description_normalizer(text):\n",
    "#    text = convert_to_lowercase(text)\n",
    "    text = strip_text(text)\n",
    "    text = re.sub('\\n', ' ', text) # converting text to one line\n",
    "#    text = re.sub('\\[.*?\\]', ' ', text) # removing square brackets\n",
    "    text = remove_http(text)\n",
    "    text = remove_image_tags(text)\n",
    "    text = remove_bracket(text)\n",
    "#    text = remove_table(text)\n",
    "#    text = remove_star(text)\n",
    "#    text = remove_punctuation(text)\n",
    "    text = remove_html(text)\n",
    "    text = remove_emoji(text)\n",
    "#    text = convert_acronyms(text)\n",
    "#    text = convert_contractions(text)\n",
    "    text = remove_stopwords(text)\n",
    "    text = remove_number_token(text)\n",
    "#    text = text_lemmatizer(text)\n",
    "#    text = discard_non_alpha(text)\n",
    "#    text = keep_pos(text)\n",
    "#    text = remove_additional_stopwords(text)\n",
    "    text = re.sub(' +', ' ', text)  # replace multiple spaces with a single space\n",
    "    text = strip_text(text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['normalized_summary'] = data['Summary'].apply(summary_normalizer)\n",
    "data['normalized_description'] = data['Description'].apply(description_normalizer)\n",
    "\n",
    "data['normalized_text'] = data['normalized_summary'] + ' ' + data['normalized_description']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "assignee_counts = data['Assignee Id'].value_counts()\n",
    "# 过滤出计数大于等于50的Assignee Id\n",
    "filtered_assignees = assignee_counts[assignee_counts >= 50].index\n",
    "# 使用isin过滤出满足条件的数据\n",
    "filtered_data = data[data['Assignee Id'].isin(filtered_assignees)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from bertopic import BERTopic\n",
    "from umap import UMAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = filtered_data['normalized_text'].to_list()\n",
    "texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_model = SentenceTransformer(\"all-mpnet-base-v2\")\n",
    "embeddings = sentence_model.encode(texts, show_progress_bar=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train BERTopic\n",
    "topic_model = BERTopic(n_gram_range=(1,3), min_topic_size=5, nr_topics=30)\n",
    "topics, probs = topic_model.fit_transform(texts, embeddings)\n",
    "#topic_model.reduce_topics(texts, nr_topics=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 查看生成的主题\n",
    "topic_model.get_topic_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将主题和可能性加入DataFrame\n",
    "filtered_data['Topic'] = topics\n",
    "filtered_data['Prob'] = [\n",
    "    max(prob) if isinstance(prob, (list, np.ndarray)) else prob \n",
    "    for prob in probs\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_data[['Assignee Id', 'Summary','normalized_summary','Description','normalized_description','Topic']].to_csv('/home/ryan/Downloads/data.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the visualization with the original embeddings\n",
    "topic_model.visualize_documents(texts, embeddings=embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "# 准备数据：按 Assignee 和 Topic 分组，并统计每个 Assignee 在每个 Topic 下的数量\n",
    "topic_distribution = filtered_data.groupby(['Assignee Id', 'Topic']).size().unstack(fill_value=0)\n",
    "\n",
    "# 使用Seaborn绘制热力图\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.heatmap(topic_distribution, cmap=\"YlGnBu\", annot=True, fmt='d')\n",
    "plt.title(\"Topic Distribution for Each Assignee\")\n",
    "plt.xlabel(\"Topic\")\n",
    "plt.ylabel(\"Assignee Id\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. 用SVM预测assignee\n",
    "# 准备特征和标签\n",
    "X = embeddings  # 使用向量化特征\n",
    "y = filtered_data['Assignee Id']  # 标签\n",
    "\n",
    "# 编码标签\n",
    "label_encoder = LabelEncoder()\n",
    "y_encoded = label_encoder.fit_transform(y)\n",
    "\n",
    "# 划分训练集和测试集，确保类别比例均衡\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_encoded, test_size=0.2, random_state=42, stratify=y_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 训练SVM模型\n",
    "svm_model = SVC(kernel='linear', probability=True)\n",
    "svm_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 评估模型\n",
    "train_score = svm_model.score(X_train, y_train)\n",
    "test_score = svm_model.score(X_test, y_test)\n",
    "\n",
    "print(f\"Train Accuracy: {train_score}\")\n",
    "print(f\"Test Accuracy: {test_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#filtered_data[['Summary','Description','Issue id', 'Assignee Id', 'normalized_summary','normalized_description', 'Topic']].to_csv('/home/ryan/Downloads/data.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import chi2_contingency\n",
    "\n",
    "# 生成 Assignee 和 Priority 的交叉表\n",
    "contingency_table = pd.crosstab(filtered_data['Assignee Id'], data['Reporter Id'])\n",
    "\n",
    "# 执行卡方检验\n",
    "chi2_stat, p, dof, expected = chi2_contingency(contingency_table)\n",
    "\n",
    "# 计算 Cramér's V\n",
    "n = contingency_table.sum().sum()  # 总样本数\n",
    "cramers_v = np.sqrt(chi2_stat / (n * (min(contingency_table.shape) - 1)))\n",
    "\n",
    "print(f\"Cramér's V: {cramers_v}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bertopic",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
