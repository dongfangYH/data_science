{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import os\n",
    "import re\n",
    "import string\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "from mlxtend.frequent_patterns import fpgrowth\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(290, 2)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"../data/JIRA_Operation_issue.csv\", usecols=['Summary', 'Description'])\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Summary</th>\n",
       "      <th>Description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5718391 - [Prod]| E38 |[Measurement/Activity M...</td>\n",
       "      <td>Hello team,\\n\\nDetails:\\nDB: mes_e38_01\\nServe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Machine Gantt Report V2 - Remove completed SN ...</td>\n",
       "      <td>Hello Team,\\n\\nBelieve a misunderstanding happ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[External Non Medical PROD] - Query on MFG Lin...</td>\n",
       "      <td>Site: Bedford PROD\\n\\nMESR Server: [42qrpt1.42...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5672686 - [PROD [S13 MES16] [LABEL-ENGINE] -SC...</td>\n",
       "      <td>Hello Antonio,\\n\\nDetails:\\nLabel name: Scrap_...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Issue with split and removal of components com...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             Summary  \\\n",
       "0  5718391 - [Prod]| E38 |[Measurement/Activity M...   \n",
       "1  Machine Gantt Report V2 - Remove completed SN ...   \n",
       "2  [External Non Medical PROD] - Query on MFG Lin...   \n",
       "3  5672686 - [PROD [S13 MES16] [LABEL-ENGINE] -SC...   \n",
       "4  Issue with split and removal of components com...   \n",
       "\n",
       "                                         Description  \n",
       "0  Hello team,\\n\\nDetails:\\nDB: mes_e38_01\\nServe...  \n",
       "1  Hello Team,\\n\\nBelieve a misunderstanding happ...  \n",
       "2  Site: Bedford PROD\\n\\nMESR Server: [42qrpt1.42...  \n",
       "3  Hello Antonio,\\n\\nDetails:\\nLabel name: Scrap_...  \n",
       "4                                                NaN  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LemmaTokenizer(object):\n",
    "\n",
    "    def __init__(self):\n",
    "        self.lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "    def __call__(self, document):\n",
    "        lemmas = []\n",
    "        \n",
    "        # Pre-proccessing of one document at the time\n",
    "        \n",
    "        # Removing puntuation\n",
    "        translator_1 = str.maketrans(string.punctuation, ' ' * len(string.punctuation))\n",
    "        document = document.translate(translator_1)\n",
    "\n",
    "        # Removing numbers\n",
    "        document = re.sub(r'\\d+', ' ', document)\n",
    "\n",
    "        # Removing special characters\n",
    "        document = re.sub(r\"[^a-zA-Z0-9]+\", ' ', document)\n",
    "\n",
    "        # The document is a string up to now, after word_tokenize(document) we'll work on every word one at the time\n",
    "        for token in word_tokenize(document):\n",
    "            \n",
    "            # Removing spaces\n",
    "            token = token.strip()\n",
    "            \n",
    "            # Lemmatizing\n",
    "            token = self.lemmatizer.lemmatize(token)\n",
    "\n",
    "            # Removing stopwords\n",
    "            if token not in stopwords and len(token) > 2:\n",
    "                lemmas.append(token)\n",
    "        return lemmas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_wordclouds(X, in_X_tfidf, k, in_word_positions):\n",
    "\n",
    "    # Clustering\n",
    "    in_model = KMeans(n_clusters=k, random_state=42, n_jobs=-1)\n",
    "    in_y_pred = in_model.fit_predict(X)\n",
    "    in_cluster_ids = set(in_y_pred)\n",
    "    silhouette_avg = silhouette_score(X, in_y_pred)\n",
    "    print(\"For n_clusters =\", k, \"The average silhouette_score is :\", silhouette_avg)\n",
    "\n",
    "    # Number of words with highest tfidf score to display\n",
    "    top_count = 100\n",
    "\n",
    "    for in_cluster_id in in_cluster_ids:\n",
    "        # compute the total tfidf for each term in the cluster\n",
    "        in_tfidf = in_X_tfidf[in_y_pred == in_cluster_id]\n",
    "        # numpy.matrix\n",
    "        tfidf_sum = np.sum(in_tfidf, axis=0)\n",
    "        # numpy.array of shape (1, X.shape[1])\n",
    "        tfidf_sum = np.asarray(tfidf_sum).reshape(-1)\n",
    "        top_indices = tfidf_sum.argsort()[-top_count:]\n",
    "        term_weights = {in_word_positions[in_idx]: tfidf_sum[in_idx] for in_idx in top_indices}\n",
    "        wc = WordCloud(width=1200, height=800, background_color=\"white\")\n",
    "        wordcloud = wc.generate_from_frequencies(term_weights)\n",
    "        fig, ax = plt.subplots(figsize=(10, 6), dpi=100)\n",
    "        ax.imshow(wordcloud, interpolation='bilinear')\n",
    "        ax.axis(\"off\")\n",
    "        fig.suptitle(f\"Cluster {in_cluster_id}\")\n",
    "        plt.show()\n",
    "\n",
    "    return in_cluster_ids\n",
    "\n",
    "# \n",
    "def custom_import_stopwords(filename):\n",
    "    in_stopword_list = []\n",
    "    in_flag = 0\n",
    "    in_word_cnt = 0\n",
    "\n",
    "    with open(filename, encoding=\"utf8\") as f:\n",
    "        for row in csv.reader(f):\n",
    "            if in_flag == 0:\n",
    "                in_flag = 1\n",
    "            else:\n",
    "                in_stopword_list.append(row[0])\n",
    "                in_word_cnt += 1\n",
    "\n",
    "    print(f\"{in_word_cnt} stopwords imported\")\n",
    "    return in_stopword_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/ryan/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9 stopwords imported\n"
     ]
    }
   ],
   "source": [
    "custom_stopwords = custom_import_stopwords('../data/custom_stopwords.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_stopwords = custom_stopwords + stopwords.words(\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      5718391 - [Prod]| E38 |[Measurement/Activity M...\n",
       "1      Machine Gantt Report V2 - Remove completed SN ...\n",
       "2      [External Non Medical PROD] - Query on MFG Lin...\n",
       "3      5672686 - [PROD [S13 MES16] [LABEL-ENGINE] -SC...\n",
       "4      Issue with split and removal of components com...\n",
       "                             ...                        \n",
       "285       Tomcat restart when adding new plants into PTS\n",
       "286          Print queues not registered in Label Engine\n",
       "287                            Issues restoring database\n",
       "288    Print queue not configured properly in the pro...\n",
       "289       Problems when creating or patching MESR DBs.  \n",
       "Name: Summary, Length: 290, dtype: object"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "contents = df['Summary']\n",
    "contents.to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom tokenizer for tfidf representation\n",
    "vectorizer = TfidfVectorizer(input='filename', tokenizer=LemmaTokenizer())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ds",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
