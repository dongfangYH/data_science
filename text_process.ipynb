{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ryan/anaconda3/envs/ds/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2024-08-01 17:08:24.803726: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "/home/ryan/anaconda3/envs/ds/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "\nBertForSequenceClassification requires the PyTorch library but it was not found in your environment.\nHowever, we were able to find a TensorFlow installation. TensorFlow classes begin\nwith \"TF\", but are otherwise identically named to our PyTorch classes. This\nmeans that the TF equivalent of the class you tried to import would be \"TFBertForSequenceClassification\".\nIf you want to use TensorFlow, please use TF classes instead!\n\nIf you really do want to use PyTorch please go to\nhttps://pytorch.org/get-started/locally/ and follow the instructions that\nmatch your environment.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m model_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbert-base-uncased\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      6\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m BertTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(model_name)\n\u001b[0;32m----> 7\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mBertForSequenceClassification\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m(model_name, num_labels\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# 定义文本分类器\u001b[39;00m\n\u001b[1;32m     10\u001b[0m classifier \u001b[38;5;241m=\u001b[39m pipeline(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext-classification\u001b[39m\u001b[38;5;124m\"\u001b[39m, model\u001b[38;5;241m=\u001b[39mmodel, tokenizer\u001b[38;5;241m=\u001b[39mtokenizer)\n",
      "File \u001b[0;32m~/anaconda3/envs/ds/lib/python3.8/site-packages/transformers/utils/import_utils.py:1070\u001b[0m, in \u001b[0;36mDummyObject.__getattribute__\u001b[0;34m(cls, key)\u001b[0m\n\u001b[1;32m   1068\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m key\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m key \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_from_config\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m   1069\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__getattribute__\u001b[39m(key)\n\u001b[0;32m-> 1070\u001b[0m \u001b[43mrequires_backends\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_backends\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/ds/lib/python3.8/site-packages/transformers/utils/import_utils.py:1049\u001b[0m, in \u001b[0;36mrequires_backends\u001b[0;34m(obj, backends)\u001b[0m\n\u001b[1;32m   1047\u001b[0m \u001b[38;5;66;03m# Raise an error for users who might not realize that classes without \"TF\" are torch-only\u001b[39;00m\n\u001b[1;32m   1048\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorch\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m backends \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtf\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m backends \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_available() \u001b[38;5;129;01mand\u001b[39;00m is_tf_available():\n\u001b[0;32m-> 1049\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(PYTORCH_IMPORT_ERROR_WITH_TF\u001b[38;5;241m.\u001b[39mformat(name))\n\u001b[1;32m   1051\u001b[0m \u001b[38;5;66;03m# Raise the inverse error for PyTorch users trying to load TF classes\u001b[39;00m\n\u001b[1;32m   1052\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtf\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m backends \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorch\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m backends \u001b[38;5;129;01mand\u001b[39;00m is_torch_available() \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_tf_available():\n",
      "\u001b[0;31mImportError\u001b[0m: \nBertForSequenceClassification requires the PyTorch library but it was not found in your environment.\nHowever, we were able to find a TensorFlow installation. TensorFlow classes begin\nwith \"TF\", but are otherwise identically named to our PyTorch classes. This\nmeans that the TF equivalent of the class you tried to import would be \"TFBertForSequenceClassification\".\nIf you want to use TensorFlow, please use TF classes instead!\n\nIf you really do want to use PyTorch please go to\nhttps://pytorch.org/get-started/locally/ and follow the instructions that\nmatch your environment.\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "from transformers import pipeline\n",
    "\n",
    "# 加载预训练的BERT模型和分词器\n",
    "model_name = \"bert-base-uncased\"\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "model = BertForSequenceClassification.from_pretrained(model_name, num_labels=2)\n",
    "\n",
    "# 定义文本分类器\n",
    "classifier = pipeline(\"text-classification\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "# 示例Jira描述内容\n",
    "description = \"\"\"\n",
    "Hi team,\n",
    "\n",
    "Could you please help to look at the case described below to improve the\n",
    "material report performance?\n",
    "\n",
    "The query took 20 minutes or more to get the data for one month.\n",
    "\n",
    "Open image-20240402-052748.png\n",
    "image-20240402-052748.png\n",
    "DB: medicalsanmhk-pts.42-q.com / PTSIII_E85\n",
    "\n",
    "2024-04-01 02:48:24 EDT [31917]: [10-1] db=PTSIII_E85,user=ptswriter,app=[unknown],client=10.191.8.52 LOG:  duration: 501045.690 ms  execute <unnamed>: SELECT                            comp.created_time,comp.barcode,inc.incoming_number,inc.coo,pn.pn_id,pn.part_number,comp.total_qty,comp.current_qty,comp.lot_code,comp.remark,comp.component_id,usr.usr_name,comp.expire_time,aml.manufacturer_id,aml.manufacturer_pn,comp.pb_category, comp.status_id,comp.lpn,aml.aml_id,aml.status_id AS aml_status_id,aml.last_update,                                              inc.incoming_id,comp.manu_info_id,comp.date_code_str,comp.date_code,co.company_name,comp.msd_level,inc.supplier_id,com.company_name\n",
    "\n",
    "                                                as supplier_name,com.vendor_code, comp.updated_date\n",
    "\n",
    "                                                FROM\n",
    "\n",
    "                                                part_number pn,component_1\n",
    "\n",
    "                                                comp,aml,company\n",
    "\n",
    "                                                co,user_info usr,incoming inc LEFT\n",
    "\n",
    "                                                JOIN company com\n",
    "\n",
    "                                                ON\n",
    "\n",
    "                                                inc.supplier_id=com.company_id\n",
    "\n",
    "                                                WHERE\n",
    "\n",
    "                                                pn.pn_id=aml.pn_id\n",
    "\n",
    "                                                AND\n",
    "\n",
    "                                                aml.aml_id=inc.aml_id\n",
    "\n",
    "                                                AND\n",
    "\n",
    "                                                aml.manufacturer_id=co.company_id\n",
    "\n",
    "                                                AND\n",
    "\n",
    "                                                usr.usr_id=comp.usr_id\n",
    "\n",
    "                                                AND\n",
    "\n",
    " inc.incoming_id=comp.incoming_id  AND comp.created_time > '1709251200'   AND comp.created_time < '1712016000'  ORDER BY comp.created_time DESC OFFSET 0 LIMIT 50000\n",
    "\n",
    "2024-04-01 03:18:49 EDT [31913]: [10-1] db=PTSIII_E85,user=ptswriter,app=[unknown],client=10.191.8.52 ERROR: canceling statement due to user request\n",
    "\n",
    "2024-04-01 03:18:49 EDT [31913]: [11-1] db=PTSIII_E85,user=ptswriter,app=[unknown],client=10.191.8.52 STATEMENT:   SELECT COUNT(*) AS total FROM ( SELECT                                 comp.created_time,comp.barcode,inc.incoming_number,inc.coo,pn.pn_id,pn.part_number,comp.total_qty,comp.current_qty,comp.lot_code,comp.remark,comp.component_id,usr.usr_name,comp.expire_time,aml.manufacturer_id,aml.manufacturer_pn,comp.pb_category, comp.status_id,comp.lpn,aml.aml_id,aml.status_id AS aml_status_id,aml.last_update,                                         inc.incoming_id,comp.manu_info_id,comp.date_code_str,comp.date_code,co.company_name,comp.msd_level,inc.supplier_id,com.company_name\n",
    "\n",
    "                                                as supplier_name,com.vendor_code, comp.updated_date\n",
    "\n",
    "                                                FROM\n",
    "\n",
    "                                                part_number pn,component_1\n",
    "\n",
    "                                                comp,aml,company\n",
    "\n",
    "                                                co,user_info usr,incoming inc LEFT\n",
    "\n",
    "                                                JOIN company com\n",
    "\n",
    "                                                ON\n",
    "\n",
    "                                                inc.supplier_id=com.company_id\n",
    "\n",
    "                                                WHERE\n",
    "\n",
    "                                                pn.pn_id=aml.pn_id\n",
    "\n",
    "                                                AND\n",
    "\n",
    "                                                aml.aml_id=inc.aml_id\n",
    "\n",
    "                                                AND\n",
    "\n",
    "                                                aml.manufacturer_id=co.company_id\n",
    "\n",
    "                                                AND\n",
    "\n",
    "                                                usr.usr_id=comp.usr_id\n",
    "\n",
    "                                                AND\n",
    "\n",
    "inc.incoming_id=comp.incoming_id  AND comp.created_time > '1709251200'   AND comp.created_time < '1712016000'  ORDER BY comp.created_time DESC ) AS totalSQLView\n",
    "\"\"\"\n",
    "\n",
    "# 分句处理\n",
    "sentences = description.split('\\n')\n",
    "\n",
    "# 过滤掉无意义的句子\n",
    "def filter_sentences(sentences, classifier):\n",
    "    meaningful_sentences = []\n",
    "    for sentence in sentences:\n",
    "        if len(sentence.strip()) > 0:\n",
    "            result = classifier(sentence)\n",
    "            if result[0]['label'] == 'LABEL_1':  # 假设LABEL_1表示有意义的句子\n",
    "                meaningful_sentences.append(sentence)\n",
    "    return meaningful_sentences\n",
    "\n",
    "# 过滤无意义的句子\n",
    "meaningful_sentences = filter_sentences(sentences, classifier)\n",
    "cleaned_description = ' '.join(meaningful_sentences)\n",
    "print(\"进一步清理后的描述内容：\")\n",
    "print(cleaned_description)\n",
    "\n",
    "# 提取关键词和特征\n",
    "def extract_keywords(text):\n",
    "    # 分词\n",
    "    words = word_tokenize(text)\n",
    "    \n",
    "    # 移除停用词\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    filtered_words = [word for word in words if word.lower() not in stop_words and word.isalnum()]\n",
    "    \n",
    "    return filtered_words\n",
    "\n",
    "# 提取特征\n",
    "keywords = extract_keywords(cleaned_description)\n",
    "print(\"提取的关键词：\")\n",
    "print(keywords)\n",
    "\n",
    "# 使用TF-IDF向量化文本内容\n",
    "vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix = vectorizer.fit_transform([cleaned_description])\n",
    "print(\"TF-IDF特征矩阵：\")\n",
    "print(tfidf_matrix.toarray())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ds",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
